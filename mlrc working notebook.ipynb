{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ccf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import LayerNorm\n",
    "import torchaudio.compliance.kaldi as ta_kaldi\n",
    "\n",
    "from backbone import (\n",
    "    TransformerEncoder,\n",
    ")\n",
    "from quantizer import (\n",
    "    NormEMAVectorQuantizer,\n",
    ")\n",
    "\n",
    "import logging\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4ed56",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TokenizersConfig:\n",
    "    def __init__(self, cfg=None):\n",
    "        self.input_patch_size: int = -1  # path size of patch embedding\n",
    "        self.embed_dim: int = 512  # patch embedding dimension\n",
    "        self.conv_bias: bool = False  # include bias in conv encoder\n",
    "\n",
    "        self.encoder_layers: int = 12  # num encoder layers in the transformer\n",
    "        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n",
    "        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n",
    "        self.encoder_attention_heads: int = 12  # num encoder attention heads\n",
    "        self.activation_fn: str = \"gelu\"  # activation function to use\n",
    "\n",
    "        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n",
    "        self.deep_norm: bool = False  # apply deep_norm first in the transformer\n",
    "\n",
    "        # dropouts\n",
    "        self.dropout: float = 0.1  # dropout probability for the transformer\n",
    "        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n",
    "        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n",
    "        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n",
    "        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n",
    "\n",
    "        # positional embeddings\n",
    "        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n",
    "        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n",
    "\n",
    "        # relative position embedding\n",
    "        self.relative_position_embedding: bool = False  # apply relative position embedding\n",
    "        self.num_buckets: int = 320  # number of buckets for relative position embedding\n",
    "        self.max_distance: int = 1280  # maximum distance for relative position embedding\n",
    "        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n",
    "\n",
    "        # quantizer\n",
    "        self.quant_n: int = 1024 # codebook number in quantizer\n",
    "        self.quant_dim: int = 256    # codebook dimension in quantizer\n",
    "\n",
    "        if cfg is not None:\n",
    "            self.update(cfg)\n",
    "\n",
    "    def update(self, cfg: dict):\n",
    "        self.__dict__.update(cfg)\n",
    "\n",
    "\n",
    "class Tokenizers(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cfg: TokenizersConfig,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        logger.info(f\"Tokenizers Config: {cfg.__dict__}\")\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embed = cfg.embed_dim\n",
    "        self.post_extract_proj = (\n",
    "            nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
    "            if self.embed != cfg.encoder_embed_dim\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.input_patch_size = cfg.input_patch_size\n",
    "        self.patch_embedding = nn.Conv2d(1, self.embed, kernel_size=self.input_patch_size, stride=self.input_patch_size,\n",
    "                                         bias=cfg.conv_bias)\n",
    "\n",
    "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
    "\n",
    "        assert not cfg.deep_norm or not cfg.layer_norm_first\n",
    "        self.encoder = TransformerEncoder(cfg)\n",
    "        self.layer_norm = LayerNorm(self.embed)\n",
    "\n",
    "        self.quantize = NormEMAVectorQuantizer(\n",
    "            n_embed=cfg.quant_n, embedding_dim=cfg.quant_dim, beta=1.0, kmeans_init=True, decay=0.99,\n",
    "        )\n",
    "        self.quant_n = cfg.quant_n\n",
    "        self.quantize_layer = nn.Sequential(\n",
    "            nn.Linear(cfg.encoder_embed_dim, cfg.encoder_embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(cfg.encoder_embed_dim, cfg.quant_dim)  # for quantize\n",
    "        )\n",
    "\n",
    "    def forward_padding_mask(\n",
    "            self,\n",
    "            features: torch.Tensor,\n",
    "            padding_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        extra = padding_mask.size(1) % features.size(1)\n",
    "        if extra > 0:\n",
    "            padding_mask = padding_mask[:, :-extra]\n",
    "        padding_mask = padding_mask.view(\n",
    "            padding_mask.size(0), features.size(1), -1\n",
    "        )\n",
    "        padding_mask = padding_mask.all(-1)\n",
    "        return padding_mask\n",
    "\n",
    "    def preprocess(\n",
    "            self,\n",
    "            source: torch.Tensor,\n",
    "            fbank_mean: float = 15.41663,\n",
    "            fbank_std: float = 6.55582,\n",
    "    ) -> torch.Tensor:\n",
    "        fbanks = []\n",
    "        for waveform in source:\n",
    "            waveform = waveform.unsqueeze(0) * 2 ** 15\n",
    "            fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)\n",
    "            fbanks.append(fbank)\n",
    "        fbank = torch.stack(fbanks, dim=0)\n",
    "        fbank = (fbank - fbank_mean) / (2 * fbank_std)\n",
    "        return fbank\n",
    "\n",
    "    def extract_labels(\n",
    "            self,\n",
    "            source: torch.Tensor,\n",
    "            padding_mask: Optional[torch.Tensor] = None,\n",
    "            fbank_mean: float = 15.41663,\n",
    "            fbank_std: float = 6.55582,\n",
    "    ):\n",
    "        fbank = self.preprocess(source, fbank_mean=fbank_mean, fbank_std=fbank_std)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = self.forward_padding_mask(fbank, padding_mask)\n",
    "\n",
    "        fbank = fbank.unsqueeze(1)\n",
    "        features = self.patch_embedding(fbank)\n",
    "        features = features.reshape(features.shape[0], features.shape[1], -1)\n",
    "        features = features.transpose(1, 2)\n",
    "        features = self.layer_norm(features)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = self.forward_padding_mask(features, padding_mask)\n",
    "\n",
    "        if self.post_extract_proj is not None:\n",
    "            features = self.post_extract_proj(features)\n",
    "\n",
    "        x = self.dropout_input(features)\n",
    "\n",
    "        x, layer_results = self.encoder(\n",
    "            x,\n",
    "            padding_mask=padding_mask,\n",
    "        )\n",
    "\n",
    "        quantize_input = self.quantize_layer(x)\n",
    "        quantize_feature, embed_loss, embed_ind = self.quantize(quantize_input)\n",
    "\n",
    "        return embed_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0342823",
   "metadata": {},
   "source": [
    "## BEATs Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ff238",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BEATsConfig:\n",
    "    def __init__(self, cfg=None):\n",
    "        self.input_patch_size: int = -1  # path size of patch embedding\n",
    "        self.embed_dim: int = 512  # patch embedding dimension\n",
    "        self.conv_bias: bool = False  # include bias in conv encoder\n",
    "\n",
    "        self.encoder_layers: int = 12  # num encoder layers in the transformer\n",
    "        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n",
    "        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n",
    "        self.encoder_attention_heads: int = 12  # num encoder attention heads\n",
    "        self.activation_fn: str = \"gelu\"  # activation function to use\n",
    "\n",
    "        self.layer_wise_gradient_decay_ratio: float = 1.0  # ratio for layer-wise gradient decay\n",
    "        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n",
    "        self.deep_norm: bool = False  # apply deep_norm first in the transformer\n",
    "\n",
    "        # dropouts\n",
    "        self.dropout: float = 0.1  # dropout probability for the transformer\n",
    "        self.attention_dropout: float = 0.1  # dropout probability for attention weights\n",
    "        self.activation_dropout: float = 0.0  # dropout probability after activation in FFN\n",
    "        self.encoder_layerdrop: float = 0.0  # probability of dropping a tarnsformer layer\n",
    "        self.dropout_input: float = 0.0  # dropout to apply to the input (after feat extr)\n",
    "\n",
    "        # positional embeddings\n",
    "        self.conv_pos: int = 128  # number of filters for convolutional positional embeddings\n",
    "        self.conv_pos_groups: int = 16  # number of groups for convolutional positional embedding\n",
    "\n",
    "        # relative position embedding\n",
    "        self.relative_position_embedding: bool = False  # apply relative position embedding\n",
    "        self.num_buckets: int = 320  # number of buckets for relative position embedding\n",
    "        self.max_distance: int = 1280  # maximum distance for relative position embedding\n",
    "        self.gru_rel_pos: bool = False  # apply gated relative position embedding\n",
    "\n",
    "        # label predictor\n",
    "        self.finetuned_model: bool = False  # whether the model is a fine-tuned model.\n",
    "        self.predictor_dropout: float = 0.1  # dropout probability for the predictor\n",
    "        self.predictor_class: int = 527  # target class number for the predictor\n",
    "\n",
    "        if cfg is not None:\n",
    "            self.update(cfg)\n",
    "\n",
    "    def update(self, cfg: dict):\n",
    "        self.__dict__.update(cfg)\n",
    "\n",
    "\n",
    "class BEATs(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cfg: BEATsConfig,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        logger.info(f\"BEATs Config: {cfg.__dict__}\")\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.embed = cfg.embed_dim\n",
    "        self.post_extract_proj = (\n",
    "            nn.Linear(self.embed, cfg.encoder_embed_dim)\n",
    "            if self.embed != cfg.encoder_embed_dim\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.input_patch_size = cfg.input_patch_size\n",
    "        self.patch_embedding = nn.Conv2d(1, self.embed, kernel_size=self.input_patch_size, stride=self.input_patch_size,\n",
    "                                         bias=cfg.conv_bias)\n",
    "\n",
    "        self.dropout_input = nn.Dropout(cfg.dropout_input)\n",
    "\n",
    "        assert not cfg.deep_norm or not cfg.layer_norm_first\n",
    "        self.encoder = TransformerEncoder(cfg)\n",
    "        self.layer_norm = LayerNorm(self.embed)\n",
    "\n",
    "        if cfg.finetuned_model:\n",
    "            self.predictor_dropout = nn.Dropout(cfg.predictor_dropout)\n",
    "            self.predictor = nn.Linear(cfg.encoder_embed_dim, cfg.predictor_class)\n",
    "        else:\n",
    "            self.predictor = None\n",
    "\n",
    "    def forward_padding_mask(\n",
    "            self,\n",
    "            features: torch.Tensor,\n",
    "            padding_mask: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        extra = padding_mask.size(1) % features.size(1)\n",
    "        if extra > 0:\n",
    "            padding_mask = padding_mask[:, :-extra]\n",
    "        padding_mask = padding_mask.view(\n",
    "            padding_mask.size(0), features.size(1), -1\n",
    "        )\n",
    "        padding_mask = padding_mask.all(-1)\n",
    "        return padding_mask\n",
    "\n",
    "    def preprocess(\n",
    "            self,\n",
    "            source: torch.Tensor,\n",
    "            fbank_mean: float = 15.41663,\n",
    "            fbank_std: float = 6.55582,\n",
    "    ) -> torch.Tensor:\n",
    "        fbanks = []\n",
    "        for waveform in source:\n",
    "            waveform = waveform.unsqueeze(0) * 2 ** 15\n",
    "            fbank = ta_kaldi.fbank(waveform, num_mel_bins=128, sample_frequency=16000, frame_length=25, frame_shift=10)\n",
    "            fbanks.append(fbank)\n",
    "        fbank = torch.stack(fbanks, dim=0)\n",
    "        fbank = (fbank - fbank_mean) / (2 * fbank_std)\n",
    "        return fbank\n",
    "\n",
    "    def extract_features(\n",
    "            self,\n",
    "            source: torch.Tensor,\n",
    "            padding_mask: Optional[torch.Tensor] = None,\n",
    "            fbank_mean: float = 15.41663,\n",
    "            fbank_std: float = 6.55582,\n",
    "    ):\n",
    "        fbank = self.preprocess(source, fbank_mean=fbank_mean, fbank_std=fbank_std)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = self.forward_padding_mask(fbank, padding_mask)\n",
    "\n",
    "        fbank = fbank.unsqueeze(1)\n",
    "        features = self.patch_embedding(fbank)\n",
    "        features = features.reshape(features.shape[0], features.shape[1], -1)\n",
    "        features = features.transpose(1, 2)\n",
    "        features = self.layer_norm(features)\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = self.forward_padding_mask(features, padding_mask)\n",
    "\n",
    "        if self.post_extract_proj is not None:\n",
    "            features = self.post_extract_proj(features)\n",
    "\n",
    "        x = self.dropout_input(features)\n",
    "\n",
    "        x, layer_results = self.encoder(\n",
    "            x,\n",
    "            padding_mask=padding_mask,\n",
    "        )\n",
    "\n",
    "        if self.predictor is not None:\n",
    "            x = self.predictor_dropout(x)\n",
    "            logits = self.predictor(x)\n",
    "\n",
    "            if padding_mask is not None and padding_mask.any():\n",
    "                logits[padding_mask] = 0\n",
    "                logits = logits.sum(dim=1)\n",
    "                logits = logits / (~padding_mask).sum(dim=1).unsqueeze(-1).expand_as(logits)\n",
    "            else:\n",
    "                logits = logits.mean(dim=1)\n",
    "\n",
    "            lprobs = torch.sigmoid(logits)\n",
    "\n",
    "            return lprobs, padding_mask\n",
    "        else:\n",
    "            return x, padding_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b9833",
   "metadata": {},
   "source": [
    "## Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "checkpoint = torch.load('path to tokenizer') \n",
    "cfg = TokenizersConfig(checkpoint['cfg'])\n",
    "BEATs_tokenizer = Tokenizers(cfg)\n",
    "BEATs_tokenizer.load_state_dict(checkpoint['model'])\n",
    "BEATs_tokenizer.eval()\n",
    "\n",
    "# tokenize the audio and generate the labels\n",
    "audio_input_16khz = torch.randn(1, 10000)\n",
    "padding_mask = torch.zeros(1, 10000).bool()\n",
    "\n",
    "labels = BEATs_tokenizer.extract_labels(audio_input_16khz, padding_mask=padding_mask)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained model\n",
    "checkpoint = torch.load('path to model')\n",
    "\n",
    "cfg = BEATsConfig(checkpoint['cfg'])\n",
    "BEATs_model = BEATs(cfg)\n",
    "BEATs_model.load_state_dict(checkpoint['model'])\n",
    "BEATs_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd678e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predicted labels of the 0th audio are ['/m/07rgkc5', '/m/09x0r', '/m/0chx_', '/m/04rlf', '/m/07qlf79'] with probability of tensor([0.2033, 0.1416, 0.0772, 0.0640, 0.0386], grad_fn=<UnbindBackward0>)\n",
      "Top 5 predicted labels of the 1th audio are ['/m/07rgkc5', '/m/09x0r', '/m/0chx_', '/m/04rlf', '/m/0cj0r'] with probability of tensor([0.2535, 0.1673, 0.0890, 0.0499, 0.0477], grad_fn=<UnbindBackward0>)\n",
      "Top 5 predicted labels of the 2th audio are ['/m/07rgkc5', '/m/09x0r', '/m/0chx_', '/m/04rlf', '/m/07yv9'] with probability of tensor([0.2738, 0.1207, 0.0927, 0.0496, 0.0446], grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# predict the classification probability of each class\n",
    "audio_input_16khz = torch.randn(10, 10000)\n",
    "padding_mask = torch.zeros(10, 10000).bool()\n",
    "\n",
    "probs = BEATs_model.extract_features(audio_input_16khz, padding_mask=padding_mask)[0]\n",
    "\n",
    "for i, (top5_label_prob, top5_label_idx) in enumerate(zip(*probs.topk(k=5))):\n",
    "    top5_label_iter1 = [checkpoint['label_dict'][label_idx.item()] for label_idx in top5_label_idx]\n",
    "    print(f'Top 5 predicted labels of the {i}th audio are {top5_label_iter1} with probability of {top5_label_prob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10507340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/m/07rgkc5', '/m/09x0r', '/m/0chx_', '/m/04rlf', '/m/07yv9']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_label_iter1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f7faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
